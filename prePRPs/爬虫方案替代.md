## FEATURE:

替代当前 AI 文档阅读助手中的爬虫方案。由于 crawl4ai 库在 macOS 系统上存在兼容性问题，无法正常工作，需要使用新的技术栈来实现网页内容爬取功能。

新的爬虫方案将采用 Scrapy + BeautifulSoup + html2markdown 的组合来实现：
- **Scrapy**: 作为主要的爬虫框架，提供强大的网页抓取、并发处理和中间件功能
- **BeautifulSoup**: 用于 HTML 解析和内容提取，处理复杂的网页结构
- **html2markdown**: 将抓取的 HTML 内容转换为 Markdown 格式，便于后续的文本处理和 RAG 流程

该方案旨在解决跨平台兼容性问题，确保爬虫模块在各种操作系统上都能稳定运行，同时保持原有的递归抓取和同域名限制功能。

技术栈：
- **爬虫框架**: Scrapy
- **HTML 解析**: BeautifulSoup4
- **格式转换**: html2markdown
- **异步处理**: asyncio (配合 Scrapy 的异步特性)
- **数据存储**: 保持与现有 Qdrant 向量数据库的集成

## EXAMPLES:

在开发过程中，我们将创建以下关键技术点的示例：

1. **Scrapy 爬虫示例**: 一个基础的 Scrapy 爬虫脚本，演示如何配置爬虫设置、定义爬取规则，并实现同域名限制功能。

2. **BeautifulSoup 内容提取示例**: 展示如何使用 BeautifulSoup 解析 HTML 页面，提取有效内容（去除导航栏、广告、脚本等无关内容），并保留文档的结构化信息。

3. **HTML 到 Markdown 转换示例**: 演示如何使用 html2markdown 将清理后的 HTML 内容转换为 Markdown 格式，保持文档的可读性和格式。

4. **递归抓取控制示例**: 一个完整的爬虫脚本，展示如何实现递归抓取所有同域名下的页面，并设置合理的深度限制和重复 URL 过滤。

5. **与现有系统集成示例**: 演示新爬虫模块如何无缝集成到现有的后端 Python 服务中，保持与前端的通信协议不变。

## DOCUMENTATION:

在开发过程中，我们将主要参考以下官方文档：

- **Scrapy**: https://docs.scrapy.org/en/latest/
- **BeautifulSoup**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **html2markdown**: https://github.com/matthewwithanm/python-markdownify
- **Python asyncio**: https://docs.python.org/3/library/asyncio.html
- **Qdrant Python Client**: https://qdrant.tech/documentation/frameworks/langchain/

## OTHER CONSIDERATIONS:

- **平台兼容性**: 新方案的首要目标是解决跨平台兼容性问题。Scrapy、BeautifulSoup 和 html2markdown 都是成熟的 Python 库，在 Windows、macOS 和 Linux 上都有良好的支持。

- **性能优化**: Scrapy 提供了强大的并发处理能力，需要合理配置并发设置以平衡抓取效率和目标网站的负载。同时要注意内存使用，避免在处理大型网站时造成内存溢出。

- **内容质量控制**: 使用 BeautifulSoup 进行内容提取时，需要设计智能的内容过滤规则，去除广告、导航、评论等无关内容，只保留文档的核心信息。

- **格式保持**: html2markdown 转换时需要保持原文档的结构信息（如标题层级、列表、链接等），确保转换后的 Markdown 文档仍然具有良好的可读性和结构性。

- **错误处理与重试**: 实现健壮的错误处理机制，包括网络超时、HTTP 错误、解析失败等情况的处理。利用 Scrapy 的重试机制和中间件功能来提高爬取的成功率。

- **反爬虫策略**: 保持原有的反爬虫措施，包括：
  - 设置合理的 User-Agent 和请求头
  - 控制请求频率和并发数
  - 遵守 robots.txt 规则
  - 实现随机延迟和代理轮换（如需要）

- **递归限制**: 严格控制爬取范围，实现域名白名单机制，防止爬虫跨域抓取。同时设置合理的深度限制和页面数量限制。

- **数据一致性**: 确保新爬虫模块产生的数据格式与现有的文本处理和 RAG 流程兼容，保持整个系统的数据流畅性。

- **配置灵活性**: 设计可配置的爬虫参数，允许用户调整抓取深度、并发数、延迟时间等设置，以适应不同网站的特性和用户需求。

- **监控和日志**: 实现详细的日志记录和爬取进度监控，方便调试和性能分析。包括成功/失败的 URL 统计、抓取时间、数据量等指标。

- **向后兼容**: 确保新爬虫模块的 API 接口与现有代码保持兼容，最小化对现有系统的影响。如果需要更改接口，提供平滑的迁移方案。
